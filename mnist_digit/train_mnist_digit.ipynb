{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train_mnist_digit.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sibCcVOAUT0A",
        "outputId": "9bb4514f-18b8-4783-9566-b46d50231d2f"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "\n",
        "        super(CNN, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3,padding=1)\n",
        "        self.conv2 = nn.Conv2d(16, 16, kernel_size=3,padding=1)\n",
        "        self.conv3 = nn.Conv2d(16, 32, kernel_size=3,padding=1)\n",
        "        self.conv4 = nn.Conv2d(32, 32, kernel_size=3,padding=1)\n",
        "        self.fc1 = nn.Linear(7*7*32, 100)\n",
        "        self.fc2 = nn.Linear(100, 10)\n",
        "        self.drop_layer = nn.Dropout(p=0.2)\n",
        "\n",
        "    def last_hidden_layer_output(self, x):\n",
        "        x = F.max_pool2d(F.relu(self.conv1(x)), 2)\n",
        "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
        "        x = self.drop_layer(F.relu(self.conv3(x)))\n",
        "        x = self.drop_layer(F.relu(self.conv4(x)))\n",
        "        x = x.view(-1, 7*7*32)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        return x\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.last_hidden_layer_output(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "mnist_train = datasets.MNIST(\"data\", train=True, download=True, transform=transforms.ToTensor())\n",
        "mnist_test = datasets.MNIST(\"data\", train=False, download=True, transform=transforms.ToTensor())\n",
        "\n",
        "train_loader = DataLoader(mnist_train, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(mnist_test, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class Flatten(nn.Module):\n",
        "    def forward(self, x):\n",
        "        return x.view(x.shape[0], -1)\n",
        "\n",
        "torch.manual_seed(2)\n",
        "torch.cuda.manual_seed(2)\n",
        "\n",
        "softmax = nn.Softmax(dim=1)\n",
        "\n",
        "def adjust_learning_rate(optimizer, epoch):\n",
        "    \"\"\"Sets the learning rate to the initial LR decayed by 2 every 10 epochs\"\"\"\n",
        "    lr = learning_rate * (0.5 ** (epoch // 10))\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "\n",
        "def enable_dropout(model):\n",
        "    \"\"\" Function to enable the dropout layers during test-time \"\"\"\n",
        "    for m in model.modules():\n",
        "        if m.__class__.__name__.startswith('Dropout'):\n",
        "            m.train()\n",
        "\n",
        "def epoch(loader, model, opt=None):\n",
        "\n",
        "    if opt:\n",
        "        model.train()\n",
        "    else:\n",
        "        model.eval()\n",
        "    \n",
        "\n",
        "    total_loss, total_err = 0., 0.\n",
        "\n",
        "    for X, y in loader:\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        yp = model(X)\n",
        "        loss = F.nll_loss(F.log_softmax(yp, dim=1), y)\n",
        "        if opt:\n",
        "            opt.zero_grad()\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "\n",
        "        total_err += (yp.max(dim=1)[1] != y).sum().item()\n",
        "        total_loss += loss.item() * X.shape[0]\n",
        "    return total_err / len(loader.dataset), total_loss / len(loader.dataset)\n",
        "\n",
        "model_cnn = CNN()\n",
        "model_cnn = model_cnn.to(device)\n",
        "\n",
        "learning_rate = 0.01\n",
        "\n",
        "opt = optim.SGD(model_cnn.parameters(), lr=learning_rate, momentum=0.9)\n",
        "\n",
        "for t in range(30):\n",
        "    adjust_learning_rate(opt, t)\n",
        "    train_err, train_loss = epoch(train_loader, model_cnn, opt)\n",
        "    test_err, test_loss = epoch(test_loader, model_cnn)\n",
        "    print(*(\"{:.6f}\".format(i) for i in (train_err, test_err)), sep=\"\\t\")\n",
        "\n",
        "torch.save(model_cnn.state_dict(), \"model_cnn_mnist_digit.pt\")\n",
        "\n",
        "model_cnn.load_state_dict(torch.load(\"model_cnn_mnist_digit.pt\"))\n",
        "model_cnn.eval()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:180.)\n",
            "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.271583\t0.034700\n",
            "0.036650\t0.026800\n",
            "0.026950\t0.020400\n",
            "0.021033\t0.016800\n",
            "0.017583\t0.014900\n",
            "0.015567\t0.013200\n",
            "0.013700\t0.013800\n",
            "0.012267\t0.012100\n",
            "0.010883\t0.012400\n",
            "0.009800\t0.010500\n",
            "0.007517\t0.009800\n",
            "0.006250\t0.009500\n",
            "0.006833\t0.009000\n",
            "0.006583\t0.008600\n",
            "0.006100\t0.009700\n",
            "0.005833\t0.008700\n",
            "0.005017\t0.008800\n",
            "0.005450\t0.009300\n",
            "0.004950\t0.009200\n",
            "0.004600\t0.008200\n",
            "0.003950\t0.007900\n",
            "0.003483\t0.008000\n",
            "0.002850\t0.008200\n",
            "0.003333\t0.008200\n",
            "0.003383\t0.008400\n",
            "0.002783\t0.008100\n",
            "0.003133\t0.007700\n",
            "0.003317\t0.008300\n",
            "0.002783\t0.007700\n",
            "0.002850\t0.007400\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CNN(\n",
              "  (conv1): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (conv3): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (conv4): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (fc1): Linear(in_features=1568, out_features=100, bias=True)\n",
              "  (fc2): Linear(in_features=100, out_features=10, bias=True)\n",
              "  (drop_layer): Dropout(p=0.2, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    }
  ]
}